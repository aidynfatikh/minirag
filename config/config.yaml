# MiniRAG Configuration File

# Model Configuration
models:
  embedding:
    name: "intfloat/e5-small-v2"
    device: "cuda"
    # Lighter alternatives for faster inference or lower memory:
    # - "intfloat/e5-base-v2" (110M params, ~440MB) - Good balance
    # - "intfloat/e5-small-v2" (33M params, ~130MB) - Fast, lightweight
    # - "sentence-transformers/all-MiniLM-L6-v2" (22M params, ~90MB) - Very fast
    # - "BAAI/bge-small-en-v1.5" (33M params, ~130MB) - Competitive quality
    # -  e5-large-v2 (335M params, ~1.3GB) - Best quality
  
  reranker:
    name: "cross-encoder/ms-marco-MiniLM-L-12-v2"
    device: "cuda"
    enabled: true
    # Lighter alternatives:
    # - "cross-encoder/ms-marco-MiniLM-L-6-v2" (22M params) - 2x faster
    # - "cross-encoder/ms-marco-TinyBERT-L-2-v2" (4M params) - Very fast
    # Current: MiniLM-L-12 (33M params) - Good balance
  
  generator:
    name: "meta-llama/Llama-3.2-3B-Instruct"
    device: "cuda"
    quantize: true
    enabled: false  # Only enable when needed
    # Lighter alternatives:
    # - "meta-llama/Llama-3.2-1B-Instruct" (1B params) - Much faster
    # - "microsoft/Phi-3-mini-4k-instruct" (3.8B params) - Good quality
    # - "google/gemma-2b-it" (2B params) - Fast inference
    # Current: Llama-3.2-3B (3B params, ~12GB quantized) - Good balance

# Indexing Configuration
indexing:
  chunk_size: 256  # words
  overlap: 32  # words
  use_summary: false
  
  # HNSW Parameters
  hnsw:
    m: 32
    ef_construction: 256
    ef_search: 50

# Search Configuration
search:
  top_k: 5
  retrieve_multiplier: 4  # Retrieve top_k * multiplier for reranking
  title_filter_multiplier: 3  # Additional multiplier when filtering by title
  
  # Hybrid Search Weights
  hybrid:
    embedding_weight: 0.5
    bm25_weight: 0.5
    candidates_per_method: 50
  
  # Section Boost
  section_boost_weight: 2.0

# Generation Configuration
generation:
  max_tokens: 800
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1
  use_history: true
  history_window: 6  # Number of previous exchanges to keep
  max_context_chunks: 8
  max_input_length: 6144
  
  # Title extraction - LLM-based document title extraction
  title_extraction:
    max_tokens: 100
    temperature: 0.1
    max_input_length: 4096
    preview_pages: 3  # Number of first pages to check for title
    preview_chars_per_page: 2000  # Characters to read from each page
    max_pages_to_check: 10  # Maximum pages to check before giving up
    increment_pages: 2  # How many additional pages to check on each retry

# Document Processing
document_processing:
  # Header Detection
  header_detection:
    font_size_multiplier: 1.2  # Threshold for header detection
    large_header_multiplier: 0.95  # Level 1 headers
    medium_header_multiplier: 1.5  # Level 2 headers
    max_header_words: 10
  
  # Chunking
  chunking:
    min_chunk_ratio: 0.5  # Minimum chunk size as ratio of target
    max_chunk_ratio: 1.5  # Maximum chunk size as ratio of target
    max_header_words: 15
    fallback_max_chars: 1000

# File Paths
paths:
  pdfs_dir: "pdfs"
  parsed_data_dir: "parsed_data"
  index_file: "vdb.index"
  logs_dir: "logs"

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  file_timestamp_format: "%Y%m%d_%H%M%S"

# Performance
performance:
  batch_size: 32
  show_progress_bar: true
  normalize_embeddings: true
